# docker-compose.prod.yml
# Production configuration for AI Chat App

version: '3.8'

services:
  backend:
    build:
      context: ./nodejs-ai-assistant
      dockerfile: Dockerfile
      target: runtime
    container_name: ai-chat-backend-prod
    restart: always
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=production
      - PORT=3000
    env_file:
      - ./nodejs-ai-assistant/.env
    networks:
      - ai-chat-network
    healthcheck:
      test: [ "CMD", "node", "-e", "http.get('http://localhost:3000/', (res) => { process.exit(res.statusCode === 200 ? 0 : 1) })" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    deploy:
      resources:
        limits:
          memory: 512M
        reservations:
          memory: 256M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

  frontend:
    build:
      context: ./react-stream-ai-assistant
      dockerfile: Dockerfile
      target: runtime
    container_name: ai-chat-frontend-prod
    restart: always
    ports:
      - "80:80"
    environment:
      - NODE_ENV=production
    env_file:
      - ./react-stream-ai-assistant/.env
    networks:
      - ai-chat-network
    healthcheck:
      test: [ "CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost/health" ]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    depends_on:
      backend:
        condition: service_healthy
    deploy:
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 64M
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "3"

networks:
  ai-chat-network:
    driver: bridge
